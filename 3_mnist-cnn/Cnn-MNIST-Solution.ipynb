{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with Convolutional Neural Network\n",
    "\n",
    "To train regular neural network with images, we have been rescaling two dimensional image into one dimensional array. This means, we are losing a lot of information about relations between the pixels, which might lead to worse performance. Convolutional Neural networks are here to counter this issue down by keeping the relations between two (or more) dimensions of the input data, all while increasing the performance.\n",
    "\n",
    "MLP\n",
    "    - only use fully connected layers\n",
    "    - only accept vectors as input\n",
    "    \n",
    "CNN\n",
    "    - also use sparsely connected layers\n",
    "\n",
    "MLPs | CNNs\n",
    "--- | ---\n",
    "<img src=\"./neural_net2.jpeg\" width=\"400\" height=\"200\" /> | <img src=\"./cnn.jpeg\" width=\"400\" height=\"200\" />\n",
    "<center>Image source: http://cs231n.github.io/convolutional-networks/</center>\n",
    "\n",
    "In CNN, we can use `parameter sharing` between neurons to dramaticaly reduce number of parameters in the network. Parameter sharing can be used, because it's clear that if something is working for the pixel in the middle of an image, it will also work for another pixel of the image.\n",
    "\n",
    "MLP | CNN\n",
    "--- | ---\n",
    "<img src=\"./mnist-dense.png\" width=\"400\" height=\"200\" /> | <img src=\"./cnn-mnist.png\" width=\"400\" height=\"200\" />\n",
    "\n",
    "\n",
    "### Convolutional Layer\n",
    "Convolutional layer is set of filters, instead of clasic neurons, that are connected to the input layer. These filters then serve as neurons and have weights on their synapsis. Filters has their own width, height (in practice height and width are the same) and depth. Depth is equal to the number of channels in the image, so if we have a colored image, the depth is three (red, green, and blue).\n",
    "\n",
    "In convulutional layer we are working with <b>convolutional windows</b>. By sliding the window over the image we get new image with enhanced edges.\n",
    "\n",
    "![cnn_widnow.gif](./cnn_widnow.gif)\n",
    "<center>Image source: http://cs231n.github.io/convolutional-networks/</center>\n",
    "\n",
    "New image is calucalted by multiplying its matrix value with corresponding filter and then summing it up.\n",
    "\n",
    "<img src=\"./applying-filter.png\" width=\"500\" height=\"300\" />\n",
    "\n",
    "$$\n",
    " RELU\\Bigg(SUM\\Bigg(\\begin{matrix}\n",
    "  0*0 & 1*1 & 0*0 \\\\\n",
    "  0*0 & 1*1 & 0*0 \\\\\n",
    "  0*0 & 1*1 & 0*0\n",
    " \\end{matrix}\\Bigg)\\Bigg) = 3 \n",
    "$$\n",
    "\n",
    "Various filters are used to enhance various edges.\n",
    "\n",
    "<img src=\"./filters.png\" width=\"450\" height=\"250\" />\n",
    "\n",
    "By applying a filter, we are reducing the size of the image (height and width), but 'cause we are using multiple filters for each layer, we are significantly increasing its depth.\n",
    "\n",
    "<img src=\"./increasgin-depth.png\" width=\"400\" height=\"200\" />\n",
    "<center>Image source: https://keunwoochoi.wordpress.com/author/keunwoochoi/page/5/</center>\n",
    "    \n",
    "Sample of apyling some filters on the image and visualizing the edges:\n",
    "<img src=\"./car-filters.png\" width=\"600\" height=\"400\" />\n",
    "<center>Image source: https://github.com/udacity/aind2-cnn/blob/master/conv-visualization/conv_visualization.ipynb</center>\n",
    "  \n",
    "Convolutional widnows have multiple parameters, that can dramaticaly change the behviour of the network:\n",
    "\n",
    "- `Widnow size` is the width and height of the sliding window\n",
    "- `Strides` offset in pixels to move the window (most commonly set to 1 or 2)\n",
    "- `Padding` in case the `Strides` is greather than one, we need to decide what to do with pixels around the edges. We have two options:\n",
    "    - get rid of them: `valid`\n",
    "    - fill with zeros: `same`\n",
    "\n",
    "### Max Pooling\n",
    "\n",
    "Pooling helps us futher decrease number of parameters and prevent overfitting by downsampling the next representation of an image. It's common to pooling layer after each convolutional layer.\n",
    "There are multiple poolings variant:\n",
    "- `max pooling` is using window to chose a max value for the new cell of a matrix\n",
    "- `average pooling` averages the values of the widnow\n",
    "- `global average pooling` averages the value over whole layer\n",
    "\n",
    "<img src=\"./maxpool.jpeg\" width=\"400\" height=\"200\" />\n",
    "<center>Image source: http://cs231n.github.io/convolutional-networks/</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MNIST database has a training set of 60000 examples.\n",
      "The MNIST database has a test set of 10000 examples.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(\"The MNIST database has a training set of %d examples.\" % len(train_images))\n",
    "print(\"The MNIST database has a test set of %d examples.\" % len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale\n",
    "Aditionaly to the rescale from <0,255> is convient for CNN to add chanel. We are using only one color for the training so the chanel is 1. In real world scenarios we will have 3 channels (RGB).\n",
    "MNIST dataset doesn't come with the chanel so we need to reshape it with one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_train_images = train_images.astype('float32') / 255\n",
    "scaled_test_images = test_images.astype('float32') / 255\n",
    "\n",
    "scaled_train_images = scaled_train_images.reshape(scaled_train_images.shape[0], 28, 28, 1)\n",
    "scaled_test_images = scaled_test_images.reshape(scaled_test_images.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer-valued labels:\n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "One-hot labels:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "print('Integer-valued labels:')\n",
    "print(train_labels[:10])\n",
    "\n",
    "# one-hot encode the labels\n",
    "train_labels = np_utils.to_categorical(train_labels, 10)\n",
    "test_labels = np_utils.to_categorical(test_labels, 10)\n",
    "\n",
    "# print first ten (one-hot) training labels\n",
    "print('One-hot labels:')\n",
    "print(test_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "We are using Adam (`adaptive moment estimation`) as an optimizer, 'cause Adam works really great with convolutional neural networks.\n",
    "\n",
    "Adam principle is in keeping the learning rate for each synapsis, and gradualy chaning it depended on the loss size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 56)        3640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 56)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 56)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2744)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               1372500   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 1,381,230\n",
      "Trainable params: 1,381,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# define the model\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu',\n",
    "                 input_shape=(28, 28, 1)))  # 28 x 28 are images from MNIST plus 1 chanel (grey)\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "# model.add(Conv2D(filters=28, kernel_size=2, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=2))  \n",
    "model.add(Conv2D(filters=56, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# summarize the model\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 23s 482us/step - loss: 0.2969 - acc: 0.9095 - val_loss: 0.0845 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08447, saving model to mnist.model.best.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 22s 451us/step - loss: 0.0983 - acc: 0.9693 - val_loss: 0.0613 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08447 to 0.06125, saving model to mnist.model.best.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 23s 489us/step - loss: 0.0693 - acc: 0.9782 - val_loss: 0.0485 - val_acc: 0.9859\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06125 to 0.04848, saving model to mnist.model.best.hdf5\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 23s 480us/step - loss: 0.0545 - acc: 0.9828 - val_loss: 0.0422 - val_acc: 0.9869\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04848 to 0.04218, saving model to mnist.model.best.hdf5\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 23s 473us/step - loss: 0.0473 - acc: 0.9852 - val_loss: 0.0384 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04218 to 0.03835, saving model to mnist.model.best.hdf5\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 23s 475us/step - loss: 0.0386 - acc: 0.9877 - val_loss: 0.0359 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03835 to 0.03587, saving model to mnist.model.best.hdf5\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 23s 477us/step - loss: 0.0344 - acc: 0.9889 - val_loss: 0.0382 - val_acc: 0.9891\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03587\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 23s 477us/step - loss: 0.0311 - acc: 0.9899 - val_loss: 0.0336 - val_acc: 0.9902\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03587 to 0.03357, saving model to mnist.model.best.hdf5\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 23s 474us/step - loss: 0.0281 - acc: 0.9912 - val_loss: 0.0361 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03357\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 23s 475us/step - loss: 0.0240 - acc: 0.9919 - val_loss: 0.0361 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03357\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.best.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "hist = model.fit(scaled_train_images, train_labels, batch_size=128, epochs=10,\n",
    "                 validation_split=0.2, callbacks=[checkpointer],\n",
    "                 verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 99.1200%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('mnist.model.best.hdf5')\n",
    "\n",
    "score = model.evaluate(scaled_test_images, test_labels, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "\n",
    "# print test accuracy\n",
    "print('Test accuracy: %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7QAAABwCAYAAAAuVjyRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb3ab89748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "start = 15\n",
    "end = 25\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in range(end-start):\n",
    "    prediction = int(model.predict(np.expand_dims(scaled_test_images[i], axis=0)).argmax())\n",
    "    display_image = test_images[i]\n",
    "    ax = fig.add_subplot(3, 12, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(display_image, cmap='gray')\n",
    "    ax.set_title(str(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of visualization\n",
    "\n",
    "When working with something abstact, like neural networks where we are working with thousands of samples, we really need to keep on visualizing the process which helps us to significantly improve architecture and find potentional issues in our code. \n",
    "\n",
    "Here is a sample of visualizing what is important for the neurons while looking at the pixels, when they are trying to decide a class.\n",
    "\n",
    "<img src=\"./softmax-weights.png\" width=\"600\" height=\"400\" />\n",
    "<center>Image source: https://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your tasks\n",
    "\n",
    "Now here are tasks that you should do to get a little more familiar with this example:\n",
    "1. Change the parameters of the neural network - sizes of the convolutional layers etc.\n",
    "2. Add another convolutional layer or uncomment the commented code in the neural network\n",
    "3. Try to \"cripple\" the network as much as you can, to have for example accuracy around 50%. E.g. try increasing the dropout layers percentage to see what they are doing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
