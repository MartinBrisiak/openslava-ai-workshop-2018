{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer perceptron neural network with MNIST\n",
    "\n",
    "MNIST is a dataset of handwriten digits - images of the digits and their labels. MNIST classification is also used as so called \"Hello world\" of machine learning. The dataset is already part of Keras, the machine learning framework we are going to use in our workshop.\n",
    "\n",
    "Lets do the import first and load the dataset.\n",
    "\n",
    "We will get:\n",
    "- training images collection that consist of three dimensional array (index * row * column) of images representation\n",
    "- training labels that represents the ground thruth of the training images [0, 1, 5, 8, 2, 0...], based on index\n",
    "- testing images collection for validation. It's same but smaller\n",
    "- testing labels is same collection as training labels, but again, smaller and used for validation of trianed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from keras.datasets import mnist\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "# load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(\"The MNIST database has a training set of %d examples.\" % len(train_images))\n",
    "print(\"The MNIST database has a test set of %d examples.\" % len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot some random samples from the dataset, to see how they are looking. You can run the cell multiple times to see different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first six training images\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i in range(6):\n",
    "    sample_index = random.randint(0, len(train_images))\n",
    "    ax = fig.add_subplot(1, 6, i+1, xticks=[], yticks=[])  # position of specific graph\n",
    "    ax.imshow(train_images[sample_index], cmap='gray')  # display image\n",
    "    ax.set_title(str(train_labels[sample_index]))  # label over image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the detailed sample image, to see exact pixel values - these are our data which we are going to use for training, and we need to understand how they are structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = random.randint(0, len(train_images))\n",
    "# plot detailed sample\n",
    "img = train_images[sample_index]\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max()/2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        ax.annotate(str(round(img[x][y], 2)), xy=(y, x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y] < thresh else 'black')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to process the dataset with a standard Dense layer, we will need to flatten the image - as it is 28x28 pixels. So the idea is to take the input 2d array and have it as 1d array. \n",
    "\n",
    "![Image rows](image_rows.png)\n",
    "\n",
    "Each neuron in the first layer will have connections to all the pixels in the array. Then our network will look something like this (except more layers):\n",
    "\n",
    "![Image rows](flatten_mlp.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale\n",
    "Images are stored in <0, 255> range represnation of single color. It's convient to keep all values in the neural netowrk in <0, 1> interval.\n",
    "We will divide every number by 255 to get the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_train_images = train_images.astype('float32') / 255\n",
    "scaled_test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode\n",
    "Mnist dataset labels comes as [0, 2, 3, 1... ] labels. But our neural network will have ten perceptrons in the last layer - each one will be doing a prediction for a specific number. To be able to train this neural network on our dataset, we need to do so called one-hot encoding of the labels - transform each label into a sparse 1d array, which will have everywhere zeroes, except one for the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "print('Integer-valued labels:')\n",
    "print(train_labels[:10])\n",
    "\n",
    "# one-hot encode the labels\n",
    "train_labels = np_utils.to_categorical(train_labels, 10)\n",
    "test_labels = np_utils.to_categorical(test_labels, 10)\n",
    "\n",
    "# print first ten (one-hot) training labels\n",
    "print('One-hot labels:')\n",
    "print(test_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "We are using simple model with only simple Dense hidden layers and rmsprop as optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# define the model\n",
    "K.clear_session()  # multiple runs of this cell could append layers to already allocated model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=scaled_train_images.shape[1:]))  # 28 x 28 shape of the image as input shape\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# summarize the model\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We are using scaled training images for the training and checkpoiting our weights to he `mnist.model.best.hdf5` file. Those weights can be later on load using `model.load()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.best.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "hist = model.fit(scaled_train_images, train_labels, batch_size=128, epochs=10,\n",
    "                 validation_split=0.2, callbacks=[checkpointer],\n",
    "                 verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "We will evaluate the trained model against the testing dataset, that the model didn't saw yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('mnist.model.best.hdf5')\n",
    "\n",
    "score = model.evaluate(scaled_test_images, test_labels, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "\n",
    "# print test accuracy\n",
    "print('Test accuracy: %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start = random.randint(0, len(test_images) - 10)\n",
    "end = start + 10\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in range(end-start):\n",
    "    display_image = test_images[start + i]\n",
    "    prediction = int(model.predict(np.expand_dims(display_image, axis=0)).argmax())\n",
    "    ax = fig.add_subplot(3, 12, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(display_image, cmap='gray')\n",
    "    ax.set_title(str(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your tasks\n",
    "\n",
    "Now here are tasks that you should do to get a little more familiar with this example:\n",
    "1. Try changing the parameters of two Dense layers in the neural network - put there smaller sizes and see its effects\n",
    "2. Remove the dropout layers from the dense network and assess how much they affect the network performance\n",
    "3. Try to \"cripple\" the network as much as you can, to have for example accuracy around 50%. This is actually not a simple task, because we have such a dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
